{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "featured-daily",
   "metadata": {},
   "source": [
    "# docshots\n",
    "\n",
    "Cropping pictures inserted in a microsoft word document enables users to hide parts of a picture that they do not want to display. The problem is that Officeâ€™s cropping tool only modifies how the cropped image is displayed in the body of the document. The original picture remains intact. Cropped portions of the image are not completely removed from the document and can be seen by others if the file is uploaded to the internet. Data leakage can occur if there is sensitive data in the trimmed areas.\n",
    "\n",
    "Docshots searches google for documents (docx) by query, downloads them and checks for images where cropping has occured.\n",
    "\n",
    "This solution uses goog.io, They have free and commercial packages available.\n",
    "\n",
    "It is advised that you run the notebook in a sandbox or vm as it does involve downloading documents unchecked from the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "composite-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import docx2txt\n",
    "import requests\n",
    "import googlesearch\n",
    "import logging\n",
    "import urllib.request\n",
    "import xml.dom.minidom                                        \n",
    "from collections import OrderedDict\n",
    "from time import sleep\n",
    "import random\n",
    "import tldextract\n",
    "import urllib\n",
    "import wget\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "increasing-transmission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv() #loads variables from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "atlantic-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = os.getenv('GOOG') #goog.io api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-agriculture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "solar-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# set the api key in headers\n",
    "headers = {\n",
    "    \"apikey\": key\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-canyon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "noted-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def googer(term, typ):\n",
    "    #uses goog.io api to search google\n",
    "    query = {\n",
    "    \"q\": term,\n",
    "    \"hl\": \"en\",\n",
    "    \"num\": 70\n",
    "    }\n",
    "    url = f\"https://api.goog.io/v1/search/\" + urllib.parse.urlencode(query)\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    results = resp.json()\n",
    "    temp_urls = []\n",
    "    for x in results['results']:\n",
    "        temp_urls.append([x['link'], typ])\n",
    "    #returns list of lists containing [url, file extension]\n",
    "    return temp_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "interested-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(searchterm):\n",
    "    final_list = []\n",
    "    url_list = []\n",
    "    \n",
    "    #can add other extensions here\n",
    "    #url_list.append(googer(searchterm + \" filetype:docm\", 'docm'))\n",
    "    #url_list.append(googer(searchterm + \" filetype:doc\", 'doc'))\n",
    "    url_list.append(googer(searchterm + \" filetype:docx\", 'docx'))\n",
    "    #print(url_list)\n",
    "    for url in [item for sublist in url_list for item in sublist]:\n",
    "        try:   \n",
    "            r = requests.get(url[0])\n",
    "            sc = r.status_code\n",
    "            if sc == 200:\n",
    "                final_list.append(url)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "    #print(searchterm + ' number of docs = ' + str(len(final_list)))\n",
    "    return final_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "finished-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filenamer(url_link, doc_name):\n",
    "    #where documents are autodownloded, a file extension is added\n",
    "    name = ''\n",
    "    if url_link.endswith(('docm', 'docx')):\n",
    "        name = doc_name\n",
    "    else:\n",
    "        name = doc_name + '.docx'\n",
    "    return name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "anonymous-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_link(url_link, doc_name):\n",
    "    #downloads file\n",
    "    if url_link.endswith(('docm', 'docx')):\n",
    "        urllib.request.urlretrieve(url_link, doc_name)\n",
    "    else:\n",
    "        wget.download(url_link, doc_name + '.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "severe-citizen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extension_check(doc, typ):\n",
    "    #where documents are autodownloded, a file extension is checked\n",
    "    name = ''\n",
    "    if str(doc).endswith(('.docx', '.docm')):\n",
    "        name = doc\n",
    "    else:\n",
    "        name = doc + '.' + typ\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "invisible-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prioritiser(urls):\n",
    "    #for each url, the document will have the images extracted and the size of the trimmed area measured\n",
    "    #error prone as some formats aren't processed  \n",
    "    priorities = {}\n",
    "    for u in urls:\n",
    "        \n",
    "        url = u[0] #url\n",
    "        typ = u[1] #file extension\n",
    "        \n",
    "        if os.path.isdir('img_folder/' + tldextract.extract(u[0]).domain) is False:\n",
    "            os.mkdir('img_folder/' + tldextract.extract(u[0]).domain)\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "            save_link(url, url.split('/')[-1]) #saves docx to folder\n",
    "            \n",
    "        \n",
    "        try:\n",
    "            #saves images into domain labeled folder\n",
    "            docx2txt.process(filenamer(url, url.split('/')[-1]), 'img_folder/' + tldextract.extract(u[0]).domain + '/')\n",
    "\n",
    "\n",
    "\n",
    "            # Open docx document\n",
    "            doc = docx.Document(url.split('/')[-1])\n",
    "\n",
    "\n",
    "            # Save all 'rId:filenames' relationships in an dictionary named rels\n",
    "            rels = {}\n",
    "\n",
    "            for r in doc.part.rels.values():\n",
    "\n",
    "                if isinstance(r._target, docx.parts.image.ImagePart):\n",
    "                    #print(docx.parts.image.ImagePart)\n",
    "                    rels[r.rId] = os.path.basename(r._target.partname)\n",
    "\n",
    "\n",
    "            # Then process your text\n",
    "            for paragraph in doc.paragraphs:\n",
    "                # If you find an image\n",
    "                if 'Graphic' in paragraph._p.xml:\n",
    "                    # Get the rId of the image\n",
    "                    for rId in rels:\n",
    "                        if rId in paragraph._p.xml:\n",
    "                            #print(paragraph._p.xml)\n",
    "\n",
    "                            dom = xml.dom.minidom.parseString(paragraph._p.xml)\n",
    "\n",
    "                            if dom.getElementsByTagName(\"a:srcRect\") != []:\n",
    "                                #print('srcRect exists')\n",
    "                                nodes = dom.getElementsByTagName(\"a:srcRect\")\n",
    "                                #print(paragraph._p.xml)\n",
    "\n",
    "                                for node in nodes:\n",
    "\n",
    "                                    shape_dict = dict(node.attributes.items())\n",
    "\n",
    "                                    try:\n",
    "                                        width_change = float(shape_dict['r'])/100000 + float(shape_dict['l'])/100000\n",
    "                                        height_change = float(shape_dict['t'])/100000 + float(shape_dict['b'])/100000\n",
    "                                        pct_change = (width_change*height_change) * float(100)\n",
    "\n",
    "                                        priorities['img_folder/' + tldextract.extract(u[0]).domain + '/' + rels[rId]] = int(pct_change)\n",
    "                                    except:\n",
    "                                        pass\n",
    "\n",
    "                            \n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logging.error('OTHER ERROR ' + str(e))\n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # returns ordered dictionary of images and a % cropped area\n",
    "    return OrderedDict(sorted(priorities.items(), key=lambda t: t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "packed-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "def docshots(term):\n",
    "    return prioritiser(get_urls(term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for documents hosed on domains that have a hackerone bug bounty progamme \n",
    "\n",
    "'''\n",
    "def bug_bounty_domains():\n",
    "    jsn = requests.get('https://raw.githubusercontent.com/disclose/diodb/master/program-list.json').json()\n",
    "    domainlist = []\n",
    "    for j in jsn:\n",
    "        parsed = tldextract.extract(j['policy_url'])\n",
    "        domainlist.append('.'.join([parsed.domain, parsed.suffix]))\n",
    "    return domainlist\n",
    "\n",
    "bug_list = bug_bounty_domains()\n",
    "random.shuffle(bug_list)\n",
    "\n",
    "for bug in bug_list:\n",
    "    try:\n",
    "        print(bug + ' ' + str(docshots('site:' + bug)))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        logging.error('HTTP ERROR ' + str(e))\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for documents hosed on .gov.[country] domains  \n",
    "\n",
    "'''\n",
    "def tld_getter():\n",
    "    tld_list = []\n",
    "    with requests.Session() as s:\n",
    "        download = s.get('https://gist.githubusercontent.com/derlin/421d2bb55018a1538271227ff6b1299d/raw/3a131d47ca322a1d001f1f79333d924672194f36/country-codes-tlds.csv')\n",
    "\n",
    "        decoded_content = download.content.decode('utf-8')\n",
    "\n",
    "        cr = csv.reader(decoded_content.splitlines(), delimiter=',')\n",
    "        \n",
    "        for country in list(cr)[1:]:\n",
    "            tld_list.append(country[1].lstrip())\n",
    "    return tld_list\n",
    "\n",
    "ctry_list = tld_getter()\n",
    "random.shuffle(ctry_list)\n",
    "\n",
    "for ctry in ctry_list:\n",
    "    try:\n",
    "        print(ctry + ' ' + str(docshots('site:' + 'gov' + ctry)))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        logging.error('HTTP ERROR ' + str(e))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "selected-september",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('img_folder/iresource/image1.png', 40), ('img_folder/iresource/image3.png', 3), ('img_folder/iresource/image4.png', 3), ('img_folder/iresource/image5.png', 3)])\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "#scrapes documents from iresource.gov.sb subdomain\n",
    "#downloads SoundWaves_MS_all_pages.docx to the pwd\n",
    "#extracts images to img_folder/iresource\n",
    "#prints ordered dictionary seen below\n",
    "#iresource/image1.png is 40% the size of the original image in the SoundWaves_MS_all_pages.docx\n",
    "\n",
    "print(docshots('site:' + 'iresource.gov.sb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with your own domain\n",
    "\n",
    "print(docshots('site:' + 'yourdomain.tld'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ordered-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aquatic-netscape",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "nuclear-sally",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-maine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-floor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-sapphire",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
